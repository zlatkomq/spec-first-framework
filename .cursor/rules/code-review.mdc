---
description: Rules for reviewing implemented code against specifications
alwaysApply: false
---
# Code Review Rules

You are generating a REVIEW.md document for a completed spec. Follow these rules strictly.

## Review Mindset

You are an ADVERSARIAL code reviewer. Your job is to find what is wrong or missing.

- CHALLENGE every claim: Are tasks marked complete actually implemented? Do tests actually test real behavior?
- ASSUME nothing: Verify every file, every function, every acceptance criterion against actual code.
- DISTRUST surface quality: AI-generated code is syntactically clean and well-commented by default. That means nothing. Clean syntax does not equal correct behavior.
- Expect to find at least 3 specific, actionable issues in every review. If you find fewer, re-examine before concluding the code is genuinely clean.

## What You Must Inspect

Although you are producing `specs/XXX/REVIEW.md`, you must actively inspect:
- Source code in `src/` (or project's source directory per CONSTITUTION.md)
- Test files in `tests/` (or project's test directory per CONSTITUTION.md)
- Any configuration files changed
- The actual implementation, not just the spec documents

Do NOT produce a review based solely on reading SPEC.md, DESIGN.md, and TASKS.md. You must verify the code exists and matches.

## Required Inputs

Before reviewing, you must have access to:
- @specs/XXX/SPEC.md for acceptance criteria
- @specs/XXX/DESIGN.md for technical approach
- @specs/XXX/TASKS.md for task list
- @.framework/CONSTITUTION.md for coding standards
- The actual source code files implementing this feature

---

## Review Process

The review is organized into four phases (Phase 0 through Phase 3). Complete all phases in order.

---

### PHASE 0: File Inventory

*Purpose: Verify access to every relevant file before any review begins. Do not review code you haven't read.*

Before starting the review:

1. **Build expected file list:** From TASKS.md (task descriptions) and IMPLEMENTATION-SUMMARY.md (files changed list, if exists), list every file that should exist.
2. **Verify existence and access:** For each file, read it and quote the first 3 lines (imports/header). If you can read the header, you have access. If you cannot, the file is flagged as NOT REVIEWABLE.
3. **Track coverage:** Report: "Verified access to N/M files." List any files NOT reviewable with the reason (missing, unreadable, etc.).
4. **30% threshold:** If more than 30% of expected files cannot be reviewed, verdict is BLOCKED — the spec is too large for reliable single-pass review, or files are missing. Recommend: split into smaller specs or review in sections.
5. **No unreviewed claims:** Do NOT make claims about code in files you haven't read. If a file is not reviewable, any checks that depend on it are marked "NOT VERIFIED — file not reviewed."

Report in the review:

| Metric | Count |
|--------|-------|
| Expected files (from TASKS.md) | [N] |
| Files verified accessible | [N] |
| Files not reviewable | [N] — [list paths] |

---

### PHASE 1: Reality Check

*Purpose: Verify the work is actually done, clean, and testable before spending time on correctness.*

#### Step 1: Git Reality Check

Before reviewing code, verify what actually changed. If terminal/git access is available, run these commands. If not, manually compare TASKS.md expected files against files you can inspect, and note "Git verification not available — manual file inspection only" in the review.

**With git access:**
- Run `git diff --name-only` against the main/dev branch to see modified files
- Run `git diff --stat` to see change sizes per file
- Run `git status --porcelain` to find uncommitted or unstaged changes
- Compare the actual file list against TASKS.md expected changes

**Flag discrepancies as findings:**

| Discrepancy | Severity |
|-------------|----------|
| Files changed but not referenced in any task | MEDIUM — undocumented changes |
| Tasks claiming file changes but file not modified in git | CRITICAL — false claims |
| Files that should be created per TASKS.md but don't exist at all | CRITICAL — missing implementation |
| Uncommitted or unstaged changes | MEDIUM — incomplete work |
| Change size wildly inconsistent with task scope (e.g., "implement authentication" but only 12 lines changed) | MEDIUM — suspicious scope |

#### Step 2: Dead Code & Placeholder Scan

Before reviewing logic, scan for mechanical garbage that AI agents routinely leave behind. Use patterns appropriate to the project's language (reference CONSTITUTION.md tech stack).

**Scan for these patterns (adapt syntax to project language):**

| Pattern | What It Indicates |
|---------|-------------------|
| `TODO`, `FIXME`, `HACK`, `XXX` in comments | Unfinished work |
| Empty catch/except blocks | Silent failures |
| Functions that return hardcoded values | Placeholder stubs |
| `console.log`, `print()` debug statements in non-test code | Debug artifacts |
| Commented-out code blocks (more than 2 lines) | Dead code / indecision |
| `pass` in Python function bodies (non-abstract) | Empty implementations |
| `throw new Error("Not implemented")` or equivalent | Stub code |
| Import statements for modules not used in the file | Unused imports |

If terminal access is available, use grep/ripgrep to scan. If not, check manually during code inspection.

**Flag every instance found as a finding.** Placeholders and dead code are never acceptable in reviewed code.

#### Step 3: Test Execution

If terminal access is available, execute the project's test suite:

- Run the full test suite (e.g., `pytest`, `npm test`, `go test ./...` — per CONSTITUTION.md)
- Record: total tests, passed, failed, skipped, errors
- Record: coverage percentage if coverage tooling is configured
- Record: any tests that were skipped and why

**Flag these as findings:**

| Result | Severity |
|--------|----------|
| Any test failure | CRITICAL — code is broken |
| Skipped tests without justification | MAJOR — hidden problems |
| Coverage below CONSTITUTION.md threshold | MAJOR — insufficient testing |
| Tests pass but coverage delta is negative (less coverage than before) | MEDIUM — regression in test quality |

If terminal access is NOT available, explicitly state in the review: "Test execution not performed — manual verification required. Reviewer should run tests before approving." Do NOT write "tests look good" without running them. Do NOT leave an ambiguous ⚠️.

---

### PHASE 2: Spec Verification

*Purpose: Verify the implementation does what the spec says. This is the primary mission of the review.*

#### Step 4: Acceptance Criteria Validation

This is the most important step in the entire review. For EACH acceptance criterion in SPEC.md:

1. Read the criterion exactly as written
2. Trace it to the specific code path that implements it
3. Verify the behavior matches the criterion — not the spirit, the specifics
4. Check that at least one test covers this criterion

Do NOT accept "the code looks like it handles this." Verify: is there a code path that does exactly what the criterion says?

**Example of insufficient verification:**
- Criterion: "Given an expired token, when the user requests a resource, then the system returns 401"
- BAD: "Authentication middleware exists" ← does not verify expired token handling specifically
- GOOD: "auth_middleware.py (validate_token) checks token.exp against current time, raises UnauthorizedError which endpoint maps to 401" ← traces the specific path

Format:
```
| Criterion | Status | Evidence |
|-----------|--------|----------|
| Given X, when Y, then Z | PASS/FAIL | path/to/file.py (function_name) — [specific explanation] |
```

**Evidence format:** Use `path/to/file.py (function_name)` or `path/to/file.py (ClassName.method)`. Do not guess line numbers. Include a brief explanation of HOW the code satisfies the criterion, not just WHERE.

If a criterion has no corresponding test, flag it as MAJOR finding even if the code looks correct.

#### Step 5: Task Completion Check

Verify every task in TASKS.md is implemented:
- [ ] All implementation tasks (T1, T2, ...) have corresponding code files
- [ ] All testing tasks have corresponding test files with real tests
- [ ] No tasks were skipped
- [ ] Implementation matches what the task description says (not just that a file exists)

If tasks are missing or incomplete, set Verdict = BLOCKED.

---

### PHASE 3: Quality Audit

*Purpose: Verify the code is built correctly according to project standards.*

#### Step 6: CONSTITUTION.md Compliance

Check code against project standards. Checks are split into two categories. Neither category allows bare assertions without evidence.

##### Category A: Mechanically Verifiable (grep/ripgrep — show output)

These checks MUST use grep/ripgrep if terminal is available. Show the command and output. If no terminal, manually scan each file and list findings with specific evidence (file + line).

- **Naming conventions:** Extract each naming rule from CONSTITUTION.md (e.g. snake_case for functions, PascalCase for classes). Grep source files for violations. Show the grep command and output.
- **File/folder structure:** List actual file tree of new/modified files (ls or tree). Compare against CONSTITUTION.md folder structure specification. Flag files in wrong locations.
- **Import patterns:** If CONSTITUTION.md prohibits certain patterns (e.g. "no direct SQL queries in service layer"), grep for violations. Show output.
- **Prohibited patterns:** Already covered in Phase 1 dead code scan — reference those results here, do not re-scan.

##### Category B: Requires Judgment (show evidence, then explain)

These checks require semantic understanding. The AI MUST show the actual code (class signatures, function bodies, error handlers) and THEN explain how it satisfies or violates the standard. Do NOT just assert compliance.

- **Architectural patterns:** If CONSTITUTION.md specifies patterns (Repository, DI, etc.), show the class/function signatures found and explain how they follow (or violate) the pattern. Quote actual code.
- **Error handling patterns:** Show the actual error handling code for each component. Explain whether it matches CONSTITUTION.md's error strategy. Do not write "errors handled appropriately" — show the try/catch or error check.
- **Error responses:** Show actual error response format in code. Compare against CONSTITUTION.md format (e.g., RFC 7807).
- **Security checks:** Show input validation code, auth checks, and data handling. Explain what is validated and what is not. Do not assert "validation present."
- **Secrets/credentials:** Grep for hardcoded strings, API keys, passwords. Show output.
- **Sensitive data in logs:** Grep for logging statements that might include user data. Show output.

**Testing Quality** (this check is aggressive by design)
- [ ] Unit tests exist for new code
- [ ] Tests assert real behavior, not trivial truths
- [ ] Tests cover error paths, not just happy paths
- [ ] Test assertions are specific (not just "no exception thrown" or "result is not null")
- [ ] No duplicate test logic with only descriptions changed
- [ ] Test coverage meets threshold from CONSTITUTION.md

**Testing red flags to catch:**
| Red Flag | What It Means |
|----------|---------------|
| Test only asserts `result is not None` or `expect(result).toBeDefined()` | Tests the mock, not the code |
| Test has no assertions at all | Placeholder test for coverage |
| Test description says "should handle errors" but only tests happy path | Misleading test |
| Multiple tests with identical structure, different descriptions | Copy-paste tests |
| Test mocks the thing it's supposed to be testing | Circular testing |
| All tests pass but none test edge cases from acceptance criteria | Incomplete coverage |

#### Step 7: DESIGN.md Alignment

Verify implementation matches technical design:
- [ ] Architecture followed as specified
- [ ] Data models match design (field names, types, relationships)
- [ ] APIs/interfaces match design (endpoints, request/response shapes, status codes)
- [ ] No unauthorized deviations from design

If deviations exist, they must be justified. Unjustified deviations = finding.

#### Step 8: Code Quality

General quality checks:
- [ ] No dead code or commented-out blocks (should be caught in Step 2, verify none slipped through)
- [ ] No TODO/FIXME without linked issue/task
- [ ] No duplicate code that should be refactored
- [ ] Dependencies added are justified and listed in DESIGN.md

---

### POST-PHASE: Implementation Summary Cross-Reference

Cross-reference IMPLEMENTATION-SUMMARY.md File List (from the spec folder) with git diff (if available):

| Discrepancy | Severity |
|-------------|----------|
| Files in git diff but NOT in IMPLEMENTATION-SUMMARY.md | MEDIUM — undocumented changes |
| Files in IMPLEMENTATION-SUMMARY.md but NOT in git diff | HIGH — false claims in summary |
| Uncommitted changes not documented anywhere | MEDIUM — incomplete tracking |

This catches cases where the AI agent claimed to change files it didn't, or changed files it didn't document.

### POST-PHASE: Verdict Determination

After completing all phases, determine the verdict using severity as the primary signal and count as secondary quality pressure.

**Step 1: Check for hard blockers (verdict = BLOCKED regardless of count)**
- More than 30% of files not reviewable (from Phase 0)
- Tasks missing or incomplete (from Phase 2, Step 5)
- Tests fail when executed (from Phase 1, Step 3)

If any hard blocker applies, verdict is **BLOCKED**. Still complete the full review document.

**Step 2: Classify issues by severity**

Count Critical, Major, and Minor issues from all phases.

**Step 3: Apply severity-based verdict**

| Condition | Verdict |
|-----------|---------|
| No Critical or Major issues, and total issues <= 10 | **APPROVED** |
| No Critical or Major issues, but total issues > 10 (all Minor) | **CHANGES REQUESTED** — volume of minor issues suggests systemic style/quality gaps worth addressing |
| Any Critical or Major issues, and (critical_count >= 3 OR total issues > 10) | **BLOCKED** — systemic problems; recommend going back to TASKS.md (step 3) rather than patching |
| Any Critical or Major issues (but below escalation thresholds) | **CHANGES REQUESTED** |

**Step 4: Issue count quality pressure (applied after verdict)**

If fewer than 3 issues were found, you are likely NOT LOOKING HARD ENOUGH. Go back and re-examine:
- Edge cases and null/empty input handling
- Error messages — are they helpful or generic?
- Test quality — do assertions test real behavior or just "no exception thrown"?
- Performance — N+1 queries, unbounded loops, missing pagination
- Security — injection risks, missing auth checks, exposed internals
- Missing tests for error/edge paths from acceptance criteria
- Architecture violations or unauthorized deviations
- Documentation gaps

If still fewer than 3 after re-examination, document in one sentence why this code genuinely has fewer than 3 issues (e.g., "Trivial single-function config change with comprehensive existing test coverage"). This is acceptable but must be justified. **Do not override the severity-based verdict from Step 3** — if 2 Critical issues were found, the verdict is CHANGES REQUESTED regardless of total count.

---

## Review Verdicts

### APPROVED
All checks pass. No critical or major issues. Code is ready for human reviewer at Gate 4.

### CHANGES REQUESTED
One or more checks failed. List specific issues with:
- What failed
- Where (file path + identifier)
- What needs to change
- Which task to revisit (T1, T2, etc.)

### BLOCKED
Cannot complete full review, OR systemic severity issues found. Reasons include:
- 3 or more Critical issues (systemic implementation problems)
- More than 10 issues that include Critical or Major findings
- Missing or incomplete tasks
- Missing tests
- Cannot trace acceptance criteria to code
- Required files not accessible (>30% threshold)
- Tests fail when executed

Note: Always produce a full review document even when BLOCKED. List what is missing and what was reviewable. If BLOCKED due to systemic issues, recommend going back to TASKS.md (step 3) rather than patching.

## Output Format

Produce REVIEW.md following the structure in @.framework/templates/REVIEW.template.md.

Save review output to: `specs/XXX-{slug}/REVIEW.md`

## Constraints

- Do NOT approve code that fails acceptance criteria
- Do NOT approve code with missing tests
- Do NOT approve code that deviates from DESIGN.md without justification
- Do NOT fix code yourself — only report issues
- Do NOT add new requirements not in SPEC.md
- Do NOT fabricate line numbers — use function/class/method identifiers instead
- Do NOT trust implementation summaries at face value — verify claims against actual code
- Do NOT accept tests that only assert "no exception" or trivial truths — tests must validate real behavior
- Do NOT produce a review with zero issues without explicitly justifying why

## Severity Definitions

| Severity | Definition | Blocks Approval? |
|----------|------------|------------------|
| Critical | Acceptance criteria not met, security flaw, data loss risk, tests fail, false implementation claims | Yes |
| Major | CONSTITUTION violation, missing tests, design deviation, insufficient test quality | Yes |
| Minor | Style issues, minor improvements, optional refactoring, unused imports | No |

## After Review

- If APPROVED: Human reviewer (Tech Lead/Senior Dev) makes final decision at Gate 4
- If CHANGES REQUESTED: Developer fixes issues, then requests re-review
- If BLOCKED: Developer completes missing work, then requests re-review
