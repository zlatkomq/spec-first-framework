---
description: Rules for implementing code from TASKS.md
globs: src/**/*
alwaysApply: false
---

# Implementation Rules

You are implementing code based on an approved task. Follow these rules strictly.

## Implementation Mindset

You are a disciplined implementer. Your job is to produce working code that exactly matches the design.

- IMPLEMENT what is specified — nothing more, nothing less.
- VERIFY your own work — do not claim something works without evidence.
- COMPLETE the full task — do not stop at "milestones" or propose "next steps" that defer work within the current task.
- NEVER lie about what you've done. If you didn't create a file, don't say you did. If you didn't run a test, don't say it passes.

## Required Inputs

Before implementing, you must have:
- Specific task from approved TASKS.md (e.g., "Implement T2")
- Access to @specs/XXX/DESIGN.md for technical approach
- Access to @.framework/CONSTITUTION.md for coding standards

If TASKS.md is not approved (Status != APPROVED), STOP and inform the user.

## Scope Control

- Implement ONLY the specified task — nothing more, nothing less
- Do NOT implement other tasks "while you're at it"
- Do NOT refactor unrelated code
- Do NOT add features not specified in DESIGN.md
- Do NOT stop partway through a task and suggest continuing later — complete it fully
- Do NOT propose "stopping points", "milestones", or "phases" within a single task — a task is atomic
- If the task is unclear, ASK for clarification before proceeding

### Allowed Supporting Changes
- You MAY make minimal supporting changes strictly required to compile/run the code:
  - Wiring new files into index/export
  - Route registration for new endpoints
  - Module registry updates
  - Import statements
  - Fixing compile errors caused by the change
- These must be listed explicitly in your summary at the end

## Halt Conditions

STOP implementation and inform the user if any of these occur:

| Condition | Action |
|-----------|--------|
| 3 consecutive failed attempts at the same sub-problem | HALT — explain what's failing and ask for guidance. Do not keep retrying the same approach. |
| Same task fails validation gates 3 times | HALT — suggest going back to TASKS.md (step 3) or DESIGN.md (step 2). The problem is likely in the spec, not the implementation. Do not keep patching. |
| Missing dependency not in DESIGN.md | HALT — do not install undocumented dependencies. Ask if the dependency should be added. |
| Ambiguous requirement in DESIGN.md | HALT — quote the ambiguous section and ask for clarification. Do not guess. |
| Required file from another task doesn't exist yet | HALT — the task has an unmet dependency. Inform the user which task must be completed first. |
| DESIGN.md contradicts CONSTITUTION.md | HALT — quote both and ask which takes precedence. |
| You cannot verify your implementation works | HALT — explain what you can't verify and why. |

After halting, wait for user direction. Do not attempt workarounds unless explicitly told to.

## Test-Accompaniment Mandate

Every implementation task MUST produce tests alongside code. The AI may write tests first, code first, or interleaved — the ORDER is not prescribed. But the OUTCOME is non-negotiable: when a task is marked complete, tests for that task's functionality must exist and pass.

- Implementation tasks produce code AND tests for that code
- Testing tasks (if separate) produce additional test coverage
- No task is complete without corresponding test coverage

### If the task is an Implementation task (code):
- Implement the specified component/function/endpoint
- Write tests that verify the implementation works correctly
- Ensure existing tests still compile (fix imports if needed)
- Tests must assert real behavior, not trivial truths

### If the task is a Testing task:

Follow this sequence:

1. **Write the test first** — create the test file with all test cases based on the acceptance criteria and DESIGN.md behavior
2. **Verify test intent** — for each test, confirm it would fail without the implementation (explain briefly in your summary why each test targets real behavior, not a tautology)
3. **Run tests if possible** — if terminal access is available, run the tests to confirm they pass against the existing implementation. If they fail, that's a finding to report, not something to silently fix in implementation code
4. **Do NOT change implementation code** unless fixing a bug the test reveals — and report that bug in your summary

Testing tasks produce tests only. If a test reveals a bug, document it; do not silently fix production code during a testing task.

## Per-Task Validation Gates

Before marking ANY task complete (updating the checkbox to [x]), ALL of these gates must pass:

1. **Tests exist**: Tests for this task's functionality ACTUALLY EXIST as files on disk
2. **Tests pass**: If terminal access is available, run the tests and confirm 100% pass. If terminal access is not available, state "Tests written but not executed — manual verification required"
3. **Implementation matches spec**: The implementation does EXACTLY what the task description specifies — no more, no less
4. **Acceptance criteria satisfied**: All acceptance criteria from SPEC.md that relate to this task are addressed
5. **No regressions**: If terminal access is available, run the full test suite. No existing tests should fail.

**If ANY gate fails: do NOT mark the task complete.** Fix the issue first. If you cannot fix it, HALT.

## Review Continuation Detection

Before starting implementation, check if REVIEW.md exists in the spec folder:

- If REVIEW.md exists with verdict **CHANGES REQUESTED** or **BLOCKED**:
  - Display the review findings (Critical and Major issues) to the user
  - Check TASKS.md for `[AI-Review]` prefixed tasks — these are action items from the previous code review
  - **Prioritize [AI-Review] tasks** before regular incomplete tasks
  - Display: "[N] review action items found. Addressing these first."
- If REVIEW.md does not exist, or verdict is APPROVED: proceed normally

## Dev Agent Record Updates

After completing each task (passing all validation gates), update the Dev Agent Record section in TASKS.md:

1. **Agent Model Used**: Record the model name and version (e.g., "claude-4-sonnet / 2026-02-10")
2. **Implementation Log**: Add entry: `- T[N]: [what was implemented] [files: path/to/file1, path/to/file2]`
3. **Decisions Made**: Add any implementation decisions with rationale (e.g., "Used bcrypt over argon2 — CONSTITUTION.md specifies bcrypt")
4. **File List**: Add all new/modified/deleted files with change type (new/modified/deleted)

This record is machine-parseable for future Jira integration and provides the audit trail for code review.

## Honesty Requirements

These rules exist because AI coding agents have specific failure modes. Follow them without exception.

**File Creation:**
- Do NOT claim you created a file unless you actually wrote it to disk
- Do NOT say "I've created X" and then show the code only in the chat — the file must exist in the file system
- After creating files, verify they exist at the expected paths

**Test Claims:**
- Do NOT say "tests pass" unless you actually ran them and they passed
- Do NOT say "tests would pass" as a substitute for running them
- If you cannot run tests, say "Tests written but not executed — manual run required"
- NEVER fabricate test output or coverage numbers

**Implementation Claims:**
- Do NOT say "this handles edge case X" unless there is a specific code path for it
- Do NOT say "error handling is in place" unless you can point to the exact try/catch or error check
- If you're unsure whether your implementation is correct, say so in the summary

**Existing Code:**
- Before creating a new utility function, check if a similar function already exists in the codebase
- Do NOT duplicate functionality that already exists — use the existing implementation
- If you find an existing function that almost works, note it in the summary and ask if you should extend it or create a new one

## Prohibited Patterns

Never produce code containing these patterns. The adversarial review will catch and flag every instance.

| Pattern | Why It's Prohibited |
|---------|---------------------|
| `TODO`, `FIXME`, `HACK`, `XXX` comments | Unfinished work is not a deliverable |
| Empty catch/except blocks | Silent failures hide bugs |
| Functions returning hardcoded values as placeholders | Stubs are not implementation |
| `console.log`, `print()` debug statements in non-test code | Debug artifacts don't belong in production |
| Commented-out code blocks (more than 2 lines) | Commit decisions, don't hedge |
| `pass` in non-abstract Python function bodies | Empty functions are not implementation |
| `throw new Error("Not implemented")` or equivalent | If it's not implemented, the task isn't done |
| Unused imports | Dead code signals carelessness |

If you find yourself wanting to write a TODO or placeholder, that means the task scope is unclear or too large. HALT and ask for clarification instead.

## Standards Compliance

### From CONSTITUTION.md
- Follow all coding standards (naming, structure, patterns)
- Use specified tech stack and libraries
- Follow file/folder structure conventions
- Apply security practices defined there

### From DESIGN.md
- Follow the architecture exactly
- Use data models as specified
- Implement APIs/interfaces as designed
- Respect component boundaries

## Code Quality

### Error Handling
- Handle errors appropriately — no silent failures
- Use meaningful error messages that help debugging
- Follow project's error handling patterns from CONSTITUTION.md

### Input Validation
- Validate all external inputs
- Fail fast with clear messages
- Never trust user input

### Testability
- Write code that can be unit tested
- Use dependency injection where appropriate
- Avoid hardcoded dependencies
- No global state unless CONSTITUTION.md allows it

### Configuration
- No hardcoded values that belong in config
- Use environment variables or config files as per CONSTITUTION.md
- Magic numbers should be named constants

## Documentation

- Add docstrings/comments for public functions/classes
- Follow documentation style from CONSTITUTION.md
- Comments explain WHY, not WHAT (code explains what)

## Output

- Save files in locations specified by CONSTITUTION.md folder structure
- Follow file naming conventions from CONSTITUTION.md
- One component/class per file unless CONSTITUTION.md says otherwise

## Checklist Before Done

- [ ] Only the specified task is implemented
- [ ] Code follows CONSTITUTION.md standards
- [ ] Architecture matches DESIGN.md
- [ ] Errors are handled appropriately
- [ ] Inputs are validated
- [ ] Code is testable
- [ ] No hardcoded configuration values
- [ ] No prohibited patterns (TODOs, placeholders, debug prints, empty blocks)
- [ ] Files saved in correct location and actually exist
- [ ] All claims in summary are truthful and verifiable

## Required Summary

At the end of every implementation, provide:

```
## Implementation Summary

**Task:** [Task ID and description]

**Files changed:**
- path/to/file1.py (created/modified)
- path/to/file2.py (created/modified)

**What was implemented:**
- [Brief description of what you did]

**Supporting changes:**
- [Any wiring, imports, registrations — or "None"]

**Tests:** (for testing tasks only)
- [Test count and what they cover]
- [Run status: "Executed — all pass" / "Executed — N failures (details below)" / "Not executed — manual run required"]

**Existing code reuse:**
- [Functions/modules found and reused — or "No existing code applicable"]

**Assumptions / Open questions:**
- [Any assumptions made, or "None"]

**Design feedback:** (optional)
- [Any observations about DESIGN.md — or "None"]
```

This summary is required for code review (Gate 4). Every claim in this summary must be verifiable against the actual files.

## Implementation Feedback

If during implementation you discover:
- A potentially better approach than DESIGN.md specifies
- A gap or ambiguity in the design
- A potential issue in the design

Document it in the "Design feedback" section of your summary.

Examples:
- "Password validation could be cleaner as a Pydantic validator instead of service method"
- "DESIGN.md doesn't specify behavior when invite code is null vs empty string"
- "The sequence diagram shows sync call but this would benefit from async"

This does NOT authorize changing the design — follow DESIGN.md as specified. The feedback captures learning for the team to consider in future iterations or Change Requests.
